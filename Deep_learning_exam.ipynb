{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNa50DIqYhtiV8UXnw+nKyS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gulzamanshaikh/Datavisualization_PGD/blob/main/Deep_learning_exam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Name: Gul Zaman\n",
        "Class: PGD Deep learning\n",
        "Batch: 5\n",
        "CNIC: 4230142084749\n"
      ],
      "metadata": {
        "id": "6_4m6LR5c5m6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Which of the following activation functions can lead to vanishing gradients?\n",
        "(i) ReLU (ii) Tanh (iii) Leaky ReLU (iv) Sigmoid  \n",
        "\n",
        "Answer:\n",
        "iv. Sigmoid and ii. Tanh"
      ],
      "metadata": {
        "id": "gQRPC1INA6e-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After training a neural network, you observe a large gap between the training accuracy (100%) and\n",
        "the test accuracy (42%). Which of the following methods is commonly used to reduce this gap?\n",
        "(i) GANs (ii) Dropout (iii) Sigmoid activation (iv) RMSprop optimizer\n",
        "\n",
        "Answer:\n",
        "\n",
        "ii. Dropout"
      ],
      "metadata": {
        "id": "qitbsqppBavT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which of the following is true about Batch normalization?\n",
        "(i) Batch norm is another way of performing dropout.\n",
        "(ii) Batch norm makes training faster.\n",
        "(iii) In Batch norm, the mean is computed over the features.\n",
        "(iv) Batch norm is a non-linear transformation to center the dataset around the or\n",
        "\n",
        "Answer:\n",
        "\n",
        "ii. Batch Norm makes training faster"
      ],
      "metadata": {
        "id": "rOmrXYF6BiM7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which of the following propositions are true about a CONV layer? (Check all that apply.)\n",
        "(i) The number of weights depends on the depth of the input volume.\n",
        "(ii) The number of biases is equal to the number of filters.\n",
        "(iii) The total number of parameters depends on the stride.\n",
        "(iv) The total number of parameters depends on the padding.\n",
        "\n",
        "Answer:\n",
        "\n",
        "(ii) The number of biases is equal to the number of filters.\n",
        "(iv) The total number of parameters depends on the padding.\n"
      ],
      "metadata": {
        "id": "Z9dd3_9qBtQO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is Error Analysis?\n",
        "(i) The process of analyzing the performance of a model through metrics such as precision, recall\n",
        "or F1-score.\n",
        "(ii) The process of scanning mis-classified examples to identify weaknesses of a model.\n",
        "(iii) The process of tuning hyper parameters to reduce the loss function during training.\n",
        "(iv) The process of identifying which parts of your model contributed to the error.\n",
        "\n",
        "Answer:\n",
        "\n",
        "ii. The process of scanning mis-classified examples to identify weakness of a model"
      ],
      "metadata": {
        "id": "gqhBrnibB5eD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ycfh7QzPD-2U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 3\n",
        "\n",
        "Answer:\n",
        "\n",
        "total layers: 4\n",
        "Dimensions of Data: 3\n",
        "Trainable Parameters: (3 nodes of input layers * 4 nodes hidden 1 layer + 4 Bias )+ (4 nodes of hidden 1 layer * 4 nodes hidden 2 layer + 4 Bias)+ (4 nodes hidden 2 layer * 1 node of out put layer + 1 Bias) = 16 + 20 + 5 = 41"
      ],
      "metadata": {
        "id": "ry0Kd3a9Qej8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fyqyA4WYUuH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1. You want to solve a classification task. You first train your network on 20 samples. Training\n",
        "converges, but the training loss is very high. You then decide to train this network on 10,000\n",
        "examples. Is your approach to fixing the problem correct? If yes, explain the most likely results of\n",
        "training with 10,000 examples. If not, give a solution to this problem."
      ],
      "metadata": {
        "id": "2uMUqTCvUzeW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: Approach to fixing the problem is correct.\n",
        "\n",
        "\n",
        "\n",
        "### 1. **Insufficient Data**:\n",
        "   - Training on just 20 samples is highly likely to result in overfitting, where the model memorizes the training data rather than learning generalizable patterns. This leads to high training loss and poor generalization to new data.\n",
        "   \n",
        "### 2. **More Data Improves Generalization**:\n",
        "   - By increasing the training set from 20 to 10,000 examples, you're providing the model with a more diverse and representative dataset. This helps the model learn the underlying patterns in the data rather than just memorizing specific examples.\n",
        "\n",
        "### 3. **Expected Results**:\n",
        "   - **Lower Training Loss**: As the model is exposed to more examples, it should learn more effectively, resulting in a lower training loss.\n",
        "   - **Better Generalization**: The model's performance on unseen data (e.g., validation or test sets) should improve, leading to better overall classification accuracy.\n",
        "   - **Longer Training Time**: With more data, the training process will take longer, but the trade-off is worth it for improved model performance.\n",
        "\n",
        "### Alternative Solutions:\n",
        "If the training loss remains high even after increasing the dataset size, other factors could be at play, such as:\n",
        "   - **Model Complexity**: The model might be too simple to capture the complexity of the data. Consider increasing the model’s capacity (e.g., adding more layers or units).\n",
        "   - **Hyperparameter Tuning**: Optimizing learning rate, batch size, or regularization techniques might be necessary to improve performance.\n",
        "   - **Data Quality**: Ensure that the data is clean, properly labeled, and relevant to the task.\n",
        "\n",
        "In summary, training with 10,000 examples is a good step toward solving the high training loss problem, as it helps the model generalize better and reduce overfitting."
      ],
      "metadata": {
        "id": "Oqm7W8j4U7FN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ib6vFNaHVGiu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2. Give two benefits of using convolutional layers instead of fully connected ones for visual tasks.\n",
        "\n",
        "Answer: Using convolutional layers instead of fully connected layers for visual tasks offers several benefits:\n",
        "\n",
        "### 1. **Spatial Hierarchy and Local Connectivity**:\n",
        "   - **Benefit**: Convolutional layers are designed to capture local features such as edges, textures, and patterns by applying filters that slide over small regions of the input image. This allows the network to recognize spatial hierarchies, where lower-level features are combined to detect higher-level patterns (e.g., shapes, objects).\n",
        "   - **Why It Matters**: Fully connected layers, on the other hand, treat all input pixels equally, without considering the spatial structure of the data, which can lead to a loss of important spatial information. Convolutional layers preserve the spatial relationships between pixels, making them much more effective for visual tasks.\n",
        "\n",
        "### 2. **Parameter Efficiency and Reduced Complexity**:\n",
        "   - **Benefit**: Convolutional layers have far fewer parameters compared to fully connected layers, especially when dealing with high-dimensional input like images. A convolutional layer reuses the same filter across the entire image, drastically reducing the number of parameters that need to be learned.\n",
        "   - **Why It Matters**: This parameter efficiency not only reduces the computational load and memory requirements but also helps mitigate the risk of overfitting, particularly when dealing with large datasets. Fully connected layers require a vast number of parameters, which can be computationally expensive and prone to overfitting when the number of training examples is limited.\n",
        "\n",
        "These two benefits—spatial awareness and parameter efficiency—make convolutional layers particularly well-suited for visual tasks like image classification, object detection, and segmentation."
      ],
      "metadata": {
        "id": "JZHqUGf-VNzd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3. As you train your model, you realize that you do not have enough data. Name the data\n",
        "augmentation techniques that can be used to overcome the shortage of data.\n",
        "\n",
        "Answer: **Data Augmentation Techniques for Limited Datasets:**\n",
        "\n",
        "1. **Random Cropping and Flipping:**\n",
        "   - **Cropping:** Randomly crop different portions of images to create new variations.\n",
        "   - **Flipping:** Horizontally or vertically flip images to introduce new perspectives.\n",
        "\n",
        "2. **Color Jitter:**\n",
        "   - Adjust brightness, contrast, saturation, and hue to simulate different lighting conditions.\n",
        "\n",
        "3. **Rotation and Scaling:**\n",
        "   - Rotate images by random angles to create variations.\n",
        "   - Scale images up or down to simulate different object sizes.\n",
        "\n",
        "4. **Noise Addition:**\n",
        "   - Add Gaussian noise, salt-and-pepper noise, or other types of noise to introduce variations.\n",
        "\n",
        "5. **Elastic Transformations:**\n",
        "   - Apply random elastic distortions to images to create more diverse variations.\n",
        "\n",
        "6. **Cutout and Random Erasing:**\n",
        "   - Randomly mask or erase portions of images to test the model's ability to handle occlusions.\n",
        "\n",
        "7. **Mixup:**\n",
        "   - Linearly interpolate between pairs of images and their corresponding labels to create new synthetic samples.\n",
        "\n",
        "8. **Mosaic:**\n",
        "   - Combine multiple images into a single image to create more complex scenes.\n",
        "\n",
        "**Choose appropriate techniques based on the nature of your data and the goals of your task.** For example, if your task involves recognizing objects in different poses, rotation and scaling might be particularly useful.\n"
      ],
      "metadata": {
        "id": "S6hCfjt2WGol"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4. Which regularization method leads to weight sparsity? Explain why.\n",
        "\n",
        "Answer: **L1 regularization** is the regularization method that leads to weight sparsity. This means that many of the model's weights become zero, resulting in a simpler and more interpretable model.\n",
        "\n",
        "**Why L1 regularization promotes sparsity:**\n",
        "\n",
        "* **Convex penalty:** L1 regularization adds a penalty term to the loss function that is proportional to the absolute value of the weights. This penalty is convex but non-smooth at zero.\n",
        "* **Discontinuity at zero:** The non-smoothness of the L1 penalty at zero creates a discontinuity that encourages the optimization algorithm to drive weights towards zero.\n",
        "* **Sparsity bias:** Compared to L2 regularization, which has a smooth quadratic penalty, L1 regularization tends to prefer sparse solutions, even when the true underlying model is not sparse.\n",
        "\n",
        "**In contrast, L2 regularization** adds a penalty term proportional to the squared magnitude of the weights. This penalty is smooth and differentiable at zero, which doesn't encourage sparsity as strongly.\n",
        "\n",
        "**Sparsity can be beneficial in several ways:**\n",
        "\n",
        "* **Feature selection:** Sparse models can help identify the most important features in a dataset.\n",
        "* **Interpretability:** Sparse models are easier to understand and explain, as fewer features are involved in making predictions.\n",
        "* **Computational efficiency:** Sparse models can be more efficient to compute and store, especially for large datasets.\n",
        "\n",
        "**However, it's important to note that L1 regularization can also lead to some drawbacks:**\n",
        "\n",
        "* **Instability:** The sparsity pattern of the model can be sensitive to small changes in the data or hyperparameters.\n",
        "* **Bias towards zero:** L1 regularization can introduce a bias towards zero, which might not be desirable in some cases.\n",
        "\n",
        "Therefore, the choice between L1 and L2 regularization depends on the specific requirements of the task and the characteristics of the data.\n"
      ],
      "metadata": {
        "id": "21deeUgpWSSQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1:\n",
        "How can you create an image classifier for fruits and vegetables using TensorFlow in Python,\n",
        "including steps for preprocessing the data, building a neural network model with multiple layers,\n",
        "training the model, and evaluating its performance?\n",
        "Dataset link: https://drive.google.com/file/d/1CGiAWso43GCsNo_faRq4jdDIlmwy7YI4/view"
      ],
      "metadata": {
        "id": "GZKUWXY0d8Om"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install tensorflow\n",
        "!pip install matplotlib\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import zipfile\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to the dataset\n",
        "dataset_path = '/content/drive/MyDrive/fruit_vegetable_dataset.zip'\n",
        "\n",
        "# Unzip the dataset\n",
        "with zipfile.ZipFile(dataset_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/fruit_vegetable_dataset')\n",
        "\n",
        "# Set paths for training and validation directories\n",
        "train_dir = '/content/fruit_vegetable_dataset/train'\n",
        "validation_dir = '/content/fruit_vegetable_dataset/validation'\n",
        "\n",
        "# Define image data generators with data augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Load images from directories\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Build the CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(train_generator.num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=1e-4),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=20,\n",
        "    validation_data=validation_generator\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(validation_generator)\n",
        "print(f\"Validation Loss: {loss}\")\n",
        "print(f\"Validation Accuracy: {accuracy}\")\n",
        "\n",
        "# Plot training & validation accuracy and loss\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0, 1])\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.ylim([0, max(history.history['loss'])])\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Loss')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "XLEXJiLNdzjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 2:\n",
        "Face Emotion detection is a one of Advanced Deep Learning Projects in Python, helps to improve your\n",
        "deep learning skills. You have to perform these taks like how to read images from directory to\n",
        "TensorFlow arrays, creating a Keras layers model, training model with images, predicting values from\n",
        "images.\n",
        "Dataset link: https://drive.google.com/file/d/14FJae0kO2hUztFr6BwjZCl2fqIMxNzJT/view"
      ],
      "metadata": {
        "id": "Jk8EnNcjeAdu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n",
        "!pip install matplotlib\n",
        "!pip install scikit-learn\n"
      ],
      "metadata": {
        "id": "X-pQ6ikpd_ne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import drive\n"
      ],
      "metadata": {
        "id": "fghf7ZHfec6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "F06ytyIdec3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "dataset_path = '/content/drive/MyDrive/FaceEmotionDataset.zip'\n",
        "with zipfile.ZipFile(dataset_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/face_emotion_dataset')\n"
      ],
      "metadata": {
        "id": "eCd7Rx2_ec04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    validation_split=0.2  # Use 20% of data for validation\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    '/content/face_emotion_dataset/train',\n",
        "    target_size=(64, 64),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    '/content/face_emotion_dataset/validation',\n",
        "    target_size=(64, 64),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")\n"
      ],
      "metadata": {
        "id": "nYoCLSYPecyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(train_generator.num_classes, activation='softmax')  # Number of classes in dataset\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "iU_MrzS3ecvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
        "    epochs=10,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_generator.samples // validation_generator.batch_size\n",
        ")\n"
      ],
      "metadata": {
        "id": "0bOwbt8MecsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(validation_generator)\n",
        "print(f'Validation Loss: {loss}')\n",
        "print(f'Validation Accuracy: {accuracy}')\n"
      ],
      "metadata": {
        "id": "YKVDzC0DecpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_emotion(image_path):\n",
        "    from tensorflow.keras.preprocessing import image\n",
        "\n",
        "    img = image.load_img(image_path, target_size=(64, 64))\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    img_array /= 255.0  # Normalize\n",
        "\n",
        "    predictions = model.predict(img_array)\n",
        "    class_idx = np.argmax(predictions[0])\n",
        "    class_labels = list(train_generator.class_indices.keys())\n",
        "    return class_labels[class_idx]\n",
        "\n",
        "# Example usage\n",
        "print(predict_emotion('/content/face_emotion_dataset/test/sample_image.jpg'))\n"
      ],
      "metadata": {
        "id": "u5fyA2fIewKn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}